{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583d1489",
   "metadata": {},
   "source": [
    "# 1 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b86ad2",
   "metadata": {},
   "source": [
    "This task will be some hands-on programming of two different ensemble learning techniques, i.e. bagging and boosting. More specifically, we will use the [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) as well as [AdaBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) provided by the scikit-learn library.\n",
    "\n",
    "As you are already familiar with loading datasets, splitting into training and test data, fitting a classifier etc., we will this time just give you hints about what steps to do. Also, we will give you some libraries to import, so that you have a good impression already about what you might need for this task. You can use additional imports of course, but please stick with the libraries we have used in this course so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad011a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Additional imports here, if needed\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df304a",
   "metadata": {},
   "source": [
    "## 1.1 Load and Split **Digits** Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaef78",
   "metadata": {},
   "source": [
    "### Task 1.1.1 Load dataset \n",
    "\n",
    "Load the [Digits Toy Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#digits-dataset) provided by Scikit-Learn and save it in the variable given below. Make sure that **all 10 digits** are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdfe78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0525e20",
   "metadata": {},
   "source": [
    "### Task 1.1.2 Save feature and target data\n",
    "\n",
    "Save the feature data from the dataset (i.e. the vectors representing the digits) and the respective target labels in two different variables *X* and *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "075926da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322257b",
   "metadata": {},
   "source": [
    "### Task 1.1.3 Split train and test data\n",
    "\n",
    "Split all data into train and test set, denoting your training set *(X_train, y_train)* and your test set *(X_test, y_test)*. We want to have 70% of the samples for training and 30% of the samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7969cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = int(len(X) * 0.7)\n",
    "X_train = X[:batchsize]\n",
    "y_train = y[:batchsize]\n",
    "X_test = X[batchsize:]\n",
    "y_test = y[batchsize:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3a37e",
   "metadata": {},
   "source": [
    "## Task 1.2 Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a684cb0",
   "metadata": {},
   "source": [
    "### Task 1.2.1 Train Classifier\n",
    "\n",
    "Create a **Bagging Classifier** object and train it on the given training data you got from task 1.1.3. You can play with different sets of parameters for the classifier (such as number of estimators etc.), but please keep the original base estimator (i.e. decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26b33171",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BaggingClassifier(n_estimators=50, max_samples=20, n_jobs=8)\n",
    "\n",
    "bc_model = bc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6aed0d",
   "metadata": {},
   "source": [
    "### Task 1.2.2 Evaluate Classifier\n",
    "\n",
    "Now it's time to see how the classifier performs. Make the prediction of target labels *y_pred* based on the test samples and print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "907a3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier on Digits Dataset: 0.7556\n"
     ]
    }
   ],
   "source": [
    "y_pred = bc_model.predict(X_test)\n",
    "\n",
    "bc_accuracy = np.sum(y_pred == y_test) / len(y_pred)\n",
    "\n",
    "print(\"Accuracy of Bagging Classifier on Digits Dataset: %.4f\" % bc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2cb57",
   "metadata": {},
   "source": [
    "Great, you're done! Now it's time to do the same few steps using the AdaBoost classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ad56",
   "metadata": {},
   "source": [
    "## Task 1.3 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890d1ce",
   "metadata": {},
   "source": [
    "### Task 1.3.1 Train Classifier\n",
    "\n",
    "Create an **AdaBoost Classifier** object and train it on the given training data you got from task 1.1.3. You can play with different sets of parameters for the classifier (such as number of estimators, learning rate etc.), but please keep the original base estimator (i.e. decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d07eab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=10, learning_rate=5., random_state=42)\n",
    "\n",
    "abc_model = abc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50442418",
   "metadata": {},
   "source": [
    "### Task 1.3.2 Evaluate Classifier\n",
    "\n",
    "Once again it's time to see how the classifier performs. Make the prediction of target labels *y_pred* based on the test samples and print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08f853f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost Classifier on Digits Dataset: 0.3685\n"
     ]
    }
   ],
   "source": [
    "y_pred = abc_model.predict(X_test)\n",
    "\n",
    "abc_accuracy = np.sum(y_pred == y_test) / len(y_pred)\n",
    "\n",
    "print(\"Accuracy of AdaBoost Classifier on Digits Dataset: %.4f\" % abc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da78ccc",
   "metadata": {},
   "source": [
    "_________________________\n",
    "\n",
    "You're done! Are the accuracy results as you had expected? If not, feel free to adjust some parameters to maximize your outcome. Either way, you might have just written your very first ensemble classifiers in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
